{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "positive-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# direct encoding of the sample sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "encoded_seq = tokenizer.encode(\"i am sentence\")\n",
    "\n",
    "# your approach\n",
    "feature_extraction = pipeline('feature-extraction', model=\"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\", \n",
    "                              tokenizer=\"T-Systems-onsite/cross-en-de-roberta-sentence-transformer\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "coral-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_extraction(\"rühe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "indian-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 17, 444, 149357, 2]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of outputs\n",
    "print(encoded_seq) # 5\n",
    "# Note that the output has a weird list output that requires to index with 0.\n",
    "print(len(features[0])) # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "internal-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "foster-attitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.740147]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([features[0][0]], [features[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "pregnant-hamburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-1.2.0.tar.gz (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 6.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (0.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (0.24.1)\n",
      "Requirement already satisfied: scipy in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (3.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2021.3.17)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from importlib-metadata->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from torchvision->sentence-transformers) (7.2.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-py3-none-any.whl size=123337 sha256=86c53bf3f50014fc817328b33c64a794c7fc66128a7140b5c2d1859eb75ca705\n",
      "  Stored in directory: /Users/aggarwalpiush/Library/Caches/pip/wheels/5a/34/6c/17406cadd88634de11a062015d04d1de556b45c9921752805a\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "received-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitler war nie selbst Fahrzeugführer \t\t Nazi nie Fahrzeugführer \t\t Score: 0.9076\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.8820\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9685\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('bert-base-german-cased')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['Hitler war nie selbst Fahrzeugführer',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome',\n",
    "             ]\n",
    "\n",
    "sentences2 = ['Nazi nie Fahrzeugführer',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "impaired-graham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2511,  0.6610,  0.5271,  ...,  0.2598, -0.3149, -0.5842],\n",
       "        [-0.0871,  0.3255, -0.0014,  ...,  0.8438,  0.2065,  0.6135],\n",
       "        [ 0.0333,  0.2432,  0.0395,  ...,  0.1317,  0.4477,  0.3811]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "backed-newsletter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7619,  0.5956,  0.3595,  ..., -0.3110,  0.0606, -0.3255],\n",
       "        [-0.1809, -0.1935,  0.2152,  ...,  0.3540, -0.1986,  0.1951],\n",
       "        [-0.0729, -0.0261,  0.0342,  ...,  0.0060,  0.3199,  0.5921]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "native-composer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-macosx_10_11_x86_64.whl (195.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 195.6 MB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorflow) (3.15.6)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (1.28.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.7.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/envs/hatememe/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=c64c9b5966c05bc2f9080ddfaea0529288afec5110f63b5f0eb8ebeda0ee3e3c\n",
      "  Stored in directory: /Users/aggarwalpiush/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-macosx_10_9_x86_64.whl size=32550 sha256=fde6fe5ec01c0acdbb3c3664a3f6dbf3f027e6157e7cc730a2ce3c20c6beab33\n",
      "  Stored in directory: /Users/aggarwalpiush/Library/Caches/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: tensorboard-plugin-wit, tensorboard-data-server, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.2.1\n",
      "    Uninstalling h5py-3.2.1:\n",
      "      Successfully uninstalled h5py-3.2.1\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 opt-einsum-3.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "measured-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 51s 0us/step\n",
      "553476096/553467096 [==============================] - 51s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "vgg_model = VGG16()\n",
    "vgg_model = Model(inputs = vgg_model.inputs, outputs = vgg_model.layers[-2].output)\n",
    "\n",
    "def extract_image_features(img_path, img_model):\n",
    "    img = load_img(img_path, target_size=(224,224))\n",
    "    img = np.array(img)\n",
    "    reshaped_img = img.reshape(1,224,224,3)\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    img_features = img_model.predict(imgx, use_multiprocessing=True)\n",
    "    img_features = img_features.reshape((4096,))\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "equivalent-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(len(extract_image_features('../data/img/10438.png', vgg_model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
